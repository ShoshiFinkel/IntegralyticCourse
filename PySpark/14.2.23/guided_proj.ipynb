{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shoshi Finkel\n",
    "#### Guided Project 14/02/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an RDD with 5000 numbers.\n",
    "data = [num for num in range(5000)]\n",
    "my_rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Print the first value.\n",
    "print(my_rdd.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12497500"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the sum of all the values in the RDD. Use the reduce function with a lambda.\n",
    "my_rdd.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Get the sum of each value minus the accumulator (the answer is 2500)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# How many partitions are there in the RDD? Print it out.\n",
    "print(my_rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the RDD as a text file.\n",
    "my_rdd.saveAsTextFile('text_file1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many partitions are there in the folder where the RDD is saved?\n",
    "number_of_files = sum(f.startswith('p') for f in os.listdir(\"text_file1\"))\n",
    "number_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another RDD from the original numbers. Set it to have a different number of partitions.\n",
    "my_rdd2 = spark.sparkContext.parallelize(data, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save it as a text file again - how many partitions are there?\n",
    "my_rdd2.saveAsTextFile('text_file2')\n",
    "num_of_files = sum(f.startswith('p') for f in os.listdir(\"text_file2\"))\n",
    "num_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartition the second RDD and save as file. Print the number of partitions.\n",
    "my_rdd3 = my_rdd2.repartition(4)\n",
    "my_rdd3.saveAsTextFile('text_file3')\n",
    "num_of_files = sum(f.startswith('p') for f in os.listdir(\"text_file3\"))\n",
    "num_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coalesce the second RDD and save as file. Print the number of partitions.\n",
    "my_rdd4 = my_rdd2.coalesce(4)\n",
    "my_rdd4.saveAsTextFile('text_file4')\n",
    "num_of_files = sum(f.startswith('p') for f in os.listdir(\"text_file4\"))\n",
    "num_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the repartitioned files with the coalesced files - what do you notice?\n",
    "# I found that coalesce() didn't shuffle the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
